# You can define a cluster containing multiple GPUs within multiple machines by defining `gpu_mapping.yaml` as follows:

# config_cluster0:
#     host_name_node0: [num_of_processes_on_GPU0, num_of_processes_on_GPU1, num_of_processes_on_GPU2, num_of_processes_on_GPU3, ..., num_of_processes_on_GPU_n]
#     host_name_node1: [num_of_processes_on_GPU0, num_of_processes_on_GPU1, num_of_processes_on_GPU2, num_of_processes_on_GPU3, ..., num_of_processes_on_GPU_n]
#     host_name_node_m: [num_of_processes_on_GPU0, num_of_processes_on_GPU1, num_of_processes_on_GPU2, num_of_processes_on_GPU3, ..., num_of_processes_on_GPU_n]


# this is used for 10 clients and 1 server training within a single machine which has 4 GPUs
mapping_default:
    ChaoyangHe-GPU-RTX2080Tix4: [1, 1, 0, 0]

# this is used for 4 clients and 1 server training within a single machine which has 4 GPUs
mapping_config1_5:
    host1: [2, 1, 1, 1]

# this is used for 10 clients and 1 server training within a single machine which has 4 GPUs
mapping_config2_11:
    host1: [3, 3, 3, 2]

# this is used for 10 clients and 1 server training within a single machine which has 8 GPUs
mapping_config3_11:
    host1: [2, 2, 2, 1, 1, 1, 1, 1]

# this is used for 4 clients and 1 server training within a single machine which has 8 GPUs, but you hope to skip the GPU device ID.
mapping_config4_5:
    host1: [1, 0, 0, 1, 1, 0, 1, 1]

# this is used for 4 clients and 1 server training using 6 machines, each machine has 2 GPUs inside, but you hope to use the second GPU.
mapping_config5_6:
    host1: [0, 1]
    host2: [0, 1]
    host3: [0, 1]
    host4: [0, 1]
    host5: [0, 1]
# this is used for 4 clients and 1 server training using 2 machines, each machine has 2 GPUs inside, but you hope to use the second GPU.
mapping_config5_2:
    gpu-worker2: [1,1]
    gpu-worker1: [2,1]

# this is used for 10 clients and 1 server training using 4 machines, each machine has 2 GPUs inside, but you hope to use the second GPU.
mapping_config5_4:
    gpu-worker2: [1,1]
    gpu-worker1: [2,1]
    gpu-worker3: [3,1]
    gpu-worker4: [1,1]

# for grpc GPU mapping
mapping_FedML_gRPC:
    hostname_node_server: [1]
    hostname_node_1: [1, 0, 0, 0]
    hostname_node_2: [1, 0, 0, 0]

# for torch RPC GPU mapping
mapping_FedML_tRPC:
    lambda-server1: [0, 0, 0, 0, 2, 2, 1, 1]
    lambda-server2: [2, 1, 1, 1, 0, 0, 0, 0]
mapping_fednlp_sp:
        lambda-server2: [2,2,2,0,0,0,0,0]
#mapping_FedML_tRPC:
#    lambda-server1: [0, 0, 0, 0, 3, 3, 3, 2]
