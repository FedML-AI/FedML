workspace: "./"

inference_image_name: "nvcr.io/nvidia/tritonserver:24.05-py3"

# If you put the model repository in $workspace/model_repository, it will be mounted to /home/fedml/models_serving/model_repository
container_run_command: "tritonserver --model-repository=/home/fedml/models_serving/model_repository"

# If your image has the repository inside it, say in /my_models_dir/model_repository, you can do:
#container_run_command: "tritonserver --model-repository=/my_models_dir/model_repository"

readiness_probe:
  path: "v2/health/ready"

port: 8000

deploy_timeout: 1600
