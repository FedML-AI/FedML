Attack:
1. ByzantineAttack: (1) zero mode (2) random mode (3) flip mode
2. DLGAttack: "Deep leakage from gradients." 
https://proceedings.neurips.cc/paper/2019/file/60a6c4002cc7b29142def8871531281a-Paper.pdf
3. InvertAttack: "Inverting gradients-how easy is it to break privacy in federated learning?." 
Advances in Neural Information Processing Systems 33 (2020): 16937-16947.
https://github.com/JonasGeiping/invertinggradients/
4. LabelFlippingAttack: "Data Poisoning Attacks Against Federated Learning Systems."
5. RevealingLabelsFromGradientsAttack: Revealing and Protecting Labels in Distributed Training


Defense:
1. BulyanDefense: "The Hidden Vulnerability of Distributed Learning in Byzantium. "
http://proceedings.mlr.press/v80/mhamdi18a/mhamdi18a.pdf
2. CClipDefense: "Byzantine-Robust Learning on Heterogeneous Datasets via Bucketing"
https://arxiv.org/pdf/2006.09365.pdf
3. GeometricMedianDefense: "Distributed statistical machine learning in adversarial settings: Byzantine gradient descent. "
https://dl.acm.org/doi/pdf/10.1145/3154503
4. KrumDefense: "Machine Learning with Adversaries: Byzantine Tolerant Gradient Descent"
https://arxiv.org/pdf/1703.02757.pdf
5. MultiKrumDefense: "Distributed momentum for byzantine-resilient stochastic gradient descent"
6. NormDiffClippingDefense: "Can You Really Backdoor Federated Learning?" 
https://arxiv.org/pdf/1911.07963.pdf 
7. RobustLearningRateDefense: "Defending against backdoors in federated learning with robust learning rate."
https://github.com/TinfoilHat0/Defending-Against-Backdoors-with-Robust-Learning-Rate
8. SoteriaDefense: "Provable defense against privacy leakage in federated learning from representation perspective." 
https://arxiv.org/pdf/2012.06043
9. SLSGDDefense: "SLSGD: Secure and efficient distributed on-device machine learning"
https://arxiv.org/pdf/1903.06996.pdf
