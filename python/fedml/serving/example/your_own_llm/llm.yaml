workspace: "./src"
entry_point: "main_entry.py"
### This folder will NOT be upload to MLOps,
### the inference server (Docker) will mount from local directory
data_cache_dir: "~/fedml_serving/model_and_config"
# If you want to install some packages
# Please write the command in the bootstrap.sh
bootstrap: config/bootstrap.sh

# If you do not have any GPU resource but want to serve the model
# Try fedmlÂ® launch platform, and Uncomment the following lines.
# ------------------------------------------------------------
# computing:
#   minimum_num_gpus: 1           # minimum # of GPUs to provision
#   maximum_cost_per_hour: $3000   # max cost per hour for your job per gpu card
#   #allow_cross_cloud_resources: true # true, false
#   #device_type: CPU              # options: GPU, CPU, hybrid
#   resource_type: A100-80G       # e.g., A100-80G,
#   # please check the resource type list by "fedml show-resource-type"
#   # or visiting URL: https://open.fedml.ai/accelerator_resource_type
# ------------------------------------------------------------
