# Research Publications


FedML’s core technology is backed by years of cutting-edge research represented in 50+ publications in ML/FL Algorithms, Security/Privacy, Systems, and Applications.

### Outline 

1. Vision Paper for High Scientific Impacts
2. System for Large-scale Distributed/Federated Training
3. Training Algorithms for FL
4. Security/privacy for FL
5. AI Applications
A Full-stack of Scientific Publications in ML Algorithms, Security/Privacy, Systems, Applications, and Visionary Impacts

## Vision Paper for High Scientific Impacts

<span style="color:red">Being visionary to find the correct problems is always the key to impactful research. </span>

[1] [Open Problems and Advances in Federated Learning](https://arxiv.org/abs/1912.04977). FnTML 2021.

[2] [Field Guide for Federated Learning](https://arxiv.org/abs/2107.06917) (Arxiv 2021)

[3] [Federated learning for Internet of Things: : Applications, Challenges, and Opportunities](https://arxiv.org/abs/2111.07494) (Arxiv 2021)

## System for Large-scale Distributed/Federated Training
<span style="color:red">Towards communication/computation/memory-efficient, resilient and robust distributed training and inferences via ML+system co-design and real-world implementation.</span>

[1] [A fundamental tradeoff between computation and communication in distributed computing](https://ieeexplore.ieee.org/abstract/document/8051074) (IEEE Transactions on Information Theory)

[2] [FedML: A Research Library and Benchmark for Federated Machine Learning](https://arxiv.org/abs/2007.13518) (NeurIPS 2020 FL Workshop, Best Paper Award)

[3] [PipeTransformer: Automated Elastic Pipelining for Distributed Training of Transformers](http://proceedings.mlr.press/v139/he21a/he21a.pdf) (ICML 2021)

[4] [Pipe-SGD: A decentralized pipelined SGD framework for distributed deep net training](https://proceedings.neurips.cc/paper/2018/file/2c6a0bae0f071cbbf0bb3d5b11d90a82-Paper.pdf) (NeurIPS 2018)

[5] [Gradiveq: Vector quantization for bandwidth-efficient gradient aggregation in distributed cnn training](https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Qhe5ua0AAAAJ&cstart=100&pagesize=100&sortby=pubdate&citation_for_view=Qhe5ua0AAAAJ:NJ774b8OgUMC) (NeurIPS 2018)

[6] [MEST: Accurate and Fast Memory-Economic Sparse Training Framework on the Edge](https://proceedings.neurips.cc/paper/2021/hash/ae3f4c649fb55c2ee3ef4d1abdb79ce5-Abstract.html) (NeurIPS 2021)

[7] [ApproxIFER: A Model-Agnostic Approach to Resilient and Robust Prediction Serving Systems](https://arxiv.org/abs/2109.09868) (NeurIPS 2021)

[8] [Lagrange Coded Computing: Optimal Design for Resiliency, Security and Privacy](https://arxiv.org/abs/1806.00939) (AISTATS 2019)

[9] [OmniLytics: A Blockchain-based Secure Data Market for Decentralized Machine Learning](https://scholar.google.com/citations?view_op=view_citation&hl=en&user=2z2camUAAAAJ&sortby=pubdate&citation_for_view=2z2camUAAAAJ:_Qo2XoVZTnwC) (ICML 2021 FL Workshop)

[10] [AsymML: An Asymmetric Decomposition Framework for Privacy-Preserving DNN Training and Inference](https://arxiv.org/abs/2110.01229) (Arxiv 2022)

[11] [Communication-aware scheduling of serial tasks for dispersed computing](https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Qhe5ua0AAAAJ&cstart=20&pagesize=80&sortby=pubdate&citation_for_view=Qhe5ua0AAAAJ:eq2jaN3J8jMC) (IEEE/ACM Transactions on Networking)

## Training Algorithms for FL
<span style="color:red">Algorithmic innovation to land distributed training and inference on the edge into the real-world system, solving challenges in efficiency, scalability, label deficiency, personalization, fairness, low-latency, straggler mitigation, etc. </span>

[1] [Group Knowledge Transfer: Federated Learning of Large CNNs at the Edge](https://arxiv.org/abs/2007.14513) (NeurIPS’20)

[2] [FedNAS (neural architecture search for FL personalization)](https://arxiv.org/abs/2004.08546) at CVPR’20 NAS Workshop

[3] [SpreadGNN: Serverless Multi-task Federated Learning for Graph Neural Networks](https://arxiv.org/abs/2106.02743) (AAAI’21)

[4] [SSFL: Tackling Label Deficiency in Federated Learning via Personalized Self-Supervision](https://arxiv.org/abs/2110.02470) (FL-AAAI’22, Best Paper Award) 

[5] [FairFed: Enabling Group Fairness in Federated Learning](https://arxiv.org/abs/2110.00857) (NeurIPS 2021 FL workshop)

[6] [Accelerated Distributed Approximate Newton Method](https://pubmed.ncbi.nlm.nih.gov/35254992/) (TNNLS Journal, 2022)

[7] [Partial Model Averaging in Federated Learning: Performance Guarantees and Benefits](https://arxiv.org/abs/2201.03789) (FL-AAAI’2022)

[8] [SPIDER: Searching Personalized Neural Architecture for Federated Learning](https://arxiv.org/abs/2112.13939) (Arxiv’ 2022)

[9] [Layer-wise Adaptive Model Aggregation for Scalable Federated Learning](https://arxiv.org/abs/2110.10302) (Arxiv’2022)

[10] [Achieving Small-Batch Accuracy with Large-Batch Scalability via Adaptive Learning Rate Adjustment](https://openreview.net/forum?id=39Q__qgCpAH) (Arxiv’ 2022)

[11] [Coded Computing for Low-Latency Federated Learning Over Wireless Edge Networks](https://ieeexplore.ieee.org/abstract/document/9252954) (IEEE Journal on Selected Areas in Communications)

[12] [Coded computation over heterogeneous clusters](https://scholar.google.com/citations?view_op=view_citation&hl=en&user=VhnTrugAAAAJ&citation_for_view=VhnTrugAAAAJ:u-x6o8ySG0sC) (IEEE Transactions on Information Theory)

[13] [Hierarchical coded gradient aggregation for learning at the edge](https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Qhe5ua0AAAAJ&cstart=20&pagesize=80&sortby=pubdate&citation_for_view=Qhe5ua0AAAAJ:DUooU5lO8OsC) (ISIT 2020)

[14] [Coded computing for federated learning at the edge](https://arxiv.org/abs/2007.03273)

[15] [Straggler mitigation in distributed matrix multiplication: Fundamental limits and optimal coding](https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Qhe5ua0AAAAJ&cstart=20&pagesize=80&sortby=pubdate&citation_for_view=Qhe5ua0AAAAJ:JoZmwDi-zQgC) (IEEE Transactions on Information Theory)


## Security/privacy for FL

<span style="color:red">Privacy-preserving, Attack, and Defense</span>

[1] [LightSecAgg: a Lightweight and Versatile Design for Secure Aggregation in Federated Learning](https://arxiv.org/abs/2109.14236) (MLSys’22)

[2] [Turbo-Aggregate: Breaking the Quadratic Aggregation Barrier in Secure Federated Learning](https://arxiv.org/abs/2002.04156) (JSAIT’21)

[3] [Securing Secure Aggregation: Mitigating Multi-Round Privacy Leakage in Federated Learning](https://arxiv.org/abs/2106.03328) (end-to-end privacy protection in FL)

[4] [A scalable approach for privacy-preserving collaborative machine learning](https://arxiv.org/abs/2011.01963) (NeurIPS 2020)

[5] [Secure aggregation for buffered asynchronous federated learning](https://arxiv.org/abs/2110.02177) (Arxiv’2021)

[6] [Basil: A Fast and Byzantine-Resilient Approach for Decentralized Training](https://arxiv.org/abs/2109.07706)

[7] [CodedReduce: A Fast and Robust Framework for Gradient Aggregation in Distributed Learning](https://arxiv.org/abs/1902.01981) (IEEE/ACM Transactions on Networking)

[8] [Verifiable Coded Computing: Towards Fast, Secure and Private Distributed Machine Learning](https://arxiv.org/abs/2107.12958) (IPDPS 2022)

[9] [CodedPrivateML: A fast and privacy-preserving framework for distributed machine learning](https://ieeexplore.ieee.org/abstract/document/9330572) (IEEE Journal on Selected Areas in Information Theory)

[10] [Byzantine-resilient secure federated learning](https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Qhe5ua0AAAAJ&cstart=20&pagesize=80&sortby=pubdate&citation_for_view=Qhe5ua0AAAAJ:HtEfBTGE9r8C) (IEEE Journal on Selected Areas in Information Theory)

[11] [Mitigating byzantine attacks in federated learning](https://www.researchgate.net/profile/Saurav-Prakash-2/publication/344678610_Mitigating_Byzantine_Attacks_in_Federated_Learning/links/609c37b292851cca5984d6b3/Mitigating-Byzantine-Attacks-in-Federated-Learning.pdf)

[12] [Secure aggregation with heterogeneous quantization in federated learning](https://arxiv.org/abs/2009.14388)

[13] [Entangled polynomial codes for secure, private, and batch distributed matrix multiplication: Breaking the” cubic” barrier](https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Qhe5ua0AAAAJ&cstart=20&pagesize=80&sortby=pubdate&citation_for_view=Qhe5ua0AAAAJ:_axFR9aDTf0C) (ISIT 2020)

[14] [Coded merkle tree: Solving data availability attacks in blockchains](https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Qhe5ua0AAAAJ&cstart=20&pagesize=80&sortby=pubdate&citation_for_view=Qhe5ua0AAAAJ:Ug5p-4gJ2f0C) (International Conference on Financial Cryptography and Data Security)

[15] [HeteroSAg: Secure Aggregation with Heterogeneous Quantization in Federated Learning](https://arxiv.org/abs/2009.14388)

[16] [Polyshard: Coded sharding achieves linearly scaling efficiency and security simultaneously](https://ieeexplore.ieee.org/abstract/document/9141331) (IEEE Transactions on Information Forensics and Security)

## AI Applications
<span style="color:red"> Besides fundamental research in FL, we also target important applications in Natural Language Processing, Computer Vision, Data Mining, and the Internet of Things (IoTs).</span>

[1] [FedNLP: Benchmarking Federated Learning Methods for Natural Language Processing Tasks](https://arxiv.org/pdf/2104.08815.pdf) NAACL 2022

[2] [FedGraphNN: A Federated Learning Benchmark System for Graph Neural Networks](https://arxiv.org/pdf/2104.07145.pdf) (ICLR 2021 workshop; KDD 2021 workshop)

[3] [FedCV: A Federated Learning Framework for Diverse Computer Vision Tasks](https://arxiv.org/pdf/2111.11066.pdf) (FL-AAAI’2022)

[4] [Federated Learning for Internet of Things](https://arxiv.org/pdf/2106.07976.pdf) (ACM Sensys’21)

[5] [MiLeNAS: Efficient Neural Architecture Search via Mixed-Level Reformulation](https://arxiv.org/abs/2003.12238) (CVPR 2020)

[6] [AutoCTS: Automated Correlated Time Series Forecasting](https://arxiv.org/abs/2112.11174) (VLDB 2022)

[7] [Coded computing for distributed graph analytics](https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Qhe5ua0AAAAJ&cstart=20&pagesize=80&sortby=pubdate&citation_for_view=Qhe5ua0AAAAJ:uJ-U7cs_P_0C) (IEEE Transactions on Information Theory)

[8] [TACC: Topology-aware coded computing for distributed graph processing](https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Qhe5ua0AAAAJ&cstart=20&pagesize=80&sortby=pubdate&citation_for_view=Qhe5ua0AAAAJ:_FM0Bhl9EiAC) (IEEE Transactions on Signal and Information Processing over Networks)

[9] [Privacy-Aware Distributed Graph-Based Semi-Supervised Learning](https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Qhe5ua0AAAAJ&cstart=20&pagesize=80&sortby=pubdate&citation_for_view=Qhe5ua0AAAAJ:5qfkUJPXOUwC) (2019 IEEE 29th International Workshop on Machine Learning for Signal Processing (MLSP)

[10] [Lightweight Image Super-Resolution with Hierarchical and Differentiable Neural Architecture Search](https://arxiv.org/abs/2105.03939) (IJCV Journal Under Review)

[11] [Collecting Indicators of Compromise from Unstructured Text of Cybersecurity Articles using Neural-Based Sequence Labelling](https://ieeexplore.ieee.org/abstract/document/8852142) (IJCNN 2019)
